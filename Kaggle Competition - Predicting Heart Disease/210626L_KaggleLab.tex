\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{float}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{enumitem}

% Avoid hyphenating long words
\hyphenpenalty=10000
\exhyphenpenalty=10000

\begin{document}

% Title Page
\begin{titlepage}
    \begin{center}
        \vspace{2cm}
        \includegraphics[width=0.4\textwidth]{University_of_Moratuwa_logo.png}
        \vspace{1.8cm}
        
        \vspace{2.0cm}
        \Huge{\textbf{Predicting Heart Disease (Playground Series S6E2)}} \\
        \vspace{1.5cm}
        \huge{CS3111 - Introduction to Machine Learning}\\
        \vspace{4cm}
        \Large{Date: \today}\\
        \vspace{1cm}
        \Large{Suriyabandara S.M.M. - 210626L}
    \end{center}
\end{titlepage}

\section*{1. Final Kaggle Public Leaderboard Score}
\textbf{Kaggle Username:} UOM\_210626L \\
\textbf{Score: 0.95305}

\section*{2. Framing \& Data Preprocessing}
The goal of this competition is to predict the presence (1) or absence (0) of heart disease given patient data. The evaluation metric is \textbf{ROC AUC}, requiring the model to output \textit{probabilities} rather than absolute classes.

\begin{itemize}[leftmargin=*]
    \item \textbf{Preventing Data Leakage}: Features from the training and test sets were strictly separated. No cross-contamination occurred during the preprocessing stages.
    \item \textbf{Target Interpretation}: The target string labels \texttt{Presence} and \texttt{Absence} were mapped to numeric labels \texttt{1} and \texttt{0} to allow the algorithms to calculate ROC AUC probabilities effectively.
    \item \textbf{Scaling \& Normalization}: Continuous numerical features (\texttt{Age}, \texttt{BP}, \texttt{Cholesterol}, \texttt{Max HR}, \texttt{ST depression}) have widely different ranges. To ensure distance calculations and optimization steps aren't biased, \texttt{StandardScaler} was applied to put them on an equal playing field (Mean 0, Variance 1).
    \item \textbf{Categorical Handling (Feature Engineering)}: Nominal features like \texttt{Chest pain type}, \texttt{Thallium}, and \texttt{EKG results} are represented as numbers in the data, but they lack mathematical meaning. \texttt{OrdinalEncoder} was utilized to explicitly treat them as categories, passing them into an algorithm natively built for categorical data.
\end{itemize}

\section*{3. Designing Experiments \& Evaluation}
To robustly test model improvements before submitting to the Kaggle Leaderboard, \textbf{Stratified K-Fold Cross-Validation (K=5)} was implemented.
\begin{itemize}[leftmargin=*]
\item Since creating a random 80/20 train-test split risks placing all hard-to-predict outliers in a single fold, Stratified K-Fold ensured the ratio of Heart Disease \texttt{Presence} vs \texttt{Absence} was identical across all 5 slices of the dataset.
\item The model's predictions were averaged directly on the \texttt{ROC AUC} metric, yielding a more reliable correlation to the hidden public leaderboard.
\end{itemize}

\section*{4. Selection and Improvement of Models}
Instead of relying on a single algorithm, \textbf{Ensemble Methods} were used to combine the strengths of different models and reduce variance:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Random Forest Classifier (Bagging)}: Averages many deep decision trees. To enforce the \textit{Decision Tree Tip for Pruning}, the \texttt{max\_depth} was restricted to 10 and \texttt{min\_samples\_split} to 20, forcing the trees to generalize rather than memorize the training data.
    \item \textbf{Histogram-Based Gradient Boosting (Boosting)}: This algorithm iteratively builds trees, where each new tree specifically tries to minimize the errors (residuals) of the previous trees.
    \begin{itemize}
        \item \textbf{Hyperparameter Tuning}: Using \texttt{RandomizedSearchCV}, optimal parameters were identified to prevent overfitting: lowering learning rate to \texttt{0.05}, restricting \texttt{max\_leaf\_nodes} to 20, and introducing strong \textbf{L2 Regularization} (\texttt{l2\_regularization=1.0}). L2 heavily penalizes massive feature weights, creating a more robust boundary.
    \end{itemize}
    \item \textbf{Soft Voting Classifier}: Finally, both algorithms were combined using a Soft-Voting mechanism (weighted 85\% to Gradient Boosting and 15\% to Random Forest). Soft voting averages the predicted \textit{probabilities} of both models, which perfectly aligns with the required ROC AUC Kaggle metric.
\end{enumerate}

\end{document}
